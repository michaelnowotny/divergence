{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:35.065578Z",
     "start_time": "2020-07-31T03:56:35.043177Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:35.856507Z",
     "start_time": "2020-07-31T03:56:35.067510Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:39.231858Z",
     "start_time": "2020-07-31T03:56:35.858577Z"
    }
   },
   "outputs": [],
   "source": [
    "from divergence import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions and Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Artificial Sample from two Normal Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example considers two different normal distributions $p$ and $q$ with\n",
    "$p = N(2, 9)$ and $q = N(1, 4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:39.304942Z",
     "start_time": "2020-07-31T03:56:39.234347Z"
    }
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# set parameters of the normal distributions p and q\n",
    "mu_p = 2\n",
    "sigma_p = 3\n",
    "mu_q = 1\n",
    "sigma_q = 2\n",
    "\n",
    "# draw samples from each normal distribution\n",
    "n = 10000\n",
    "\n",
    "def draw_normal(mu, sigma, n: int, antithetic: bool = False):\n",
    "    z = np.random.randn(n)\n",
    "    if antithetic: \n",
    "        z = np.hstack((z, -z))\n",
    "    \n",
    "    return mu + sigma * z\n",
    "\n",
    "sample_p = draw_normal(mu_p, sigma_p, n=n, antithetic=True)\n",
    "sample_q = draw_normal(mu_q, sigma_q, n=n, antithetic=True)\n",
    "\n",
    "# fit a non-parametric density estimate for both distributions\n",
    "kde_p = sm.nonparametric.KDEUnivariate(sample_p)\n",
    "kde_q = sm.nonparametric.KDEUnivariate(sample_q)\n",
    "kde_p.fit()\n",
    "kde_q.fit()\n",
    "\n",
    "# construct exact normal densities for p and q\n",
    "pdf_p = lambda x: sp.stats.norm.pdf(x, mu_p, sigma_p)\n",
    "pdf_q = lambda x: sp.stats.norm.pdf(x, mu_q, sigma_q)\n",
    "\n",
    "# compute support for kernel density estimates\n",
    "p_min = min(kde_p.support)\n",
    "p_max = max(kde_p.support)\n",
    "q_min = min(kde_q.support)\n",
    "q_max = max(kde_q.support)\n",
    "combined_min = min(p_min, q_min)\n",
    "combined_max = max(p_max, q_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Sample from Multinomial Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:39.339814Z",
     "start_time": "2020-07-31T03:56:39.306432Z"
    }
   },
   "outputs": [],
   "source": [
    "multinomial_sample_q = np.array([1, 2, 3, 2, 3, 3, 3, 2, 1, 1])\n",
    "multinomial_sample_p = np.array([1, 2, 3, 3, 3, 3, 3, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a probability distribution $p$ is defined as \n",
    "\n",
    "$H(X) = - \\mathbb{E}_p \\left[ \\log_{\\text{base}} p \\right]$, \n",
    "\n",
    "where $\\mathbb{E}_P$ denotes expectation with respect the probability distribution $p$. In information theory, the base of the logarithm is 2 and the interpretation of entropy is the average number of bits needed to optimally encode the signal represented by the distribution $p$. \n",
    "\n",
    "Divergence defaults to $\\text{base}=e$, which results in the natural logarithm i.e. $\\log_e = \\ln$. This default choice can be overridden via the argument 'base' during the entropy calculation. In particular, specifying $\\text{base}=2$ results in the classical Shannon entropy expressed in bits, whereas specifying $\\text{base}=10$ produces the entropy in decimal bits (dits or Hartleys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy from Samples (via Statsmodels KDE Objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:39.517682Z",
     "start_time": "2020-07-31T03:56:39.341601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.531109986651922\n",
      "Entropy of q = 2.123343378353565\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {entropy_from_samples(sample_p, discrete=False)}')\n",
    "print(f'Entropy of q = {entropy_from_samples(sample_q, discrete=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy from Statsmodels KDE Objects (via Statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:39.953586Z",
     "start_time": "2020-07-31T03:56:39.520153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.531114322639585\n",
      "Entropy of q = 2.1233454054445\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {kde_p.entropy}')\n",
    "print(f'Entropy of q = {kde_q.entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy from Statsmodels KDE Objects (via Divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:40.127364Z",
     "start_time": "2020-07-31T03:56:39.957380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.531109986651922\n",
      "Entropy of q = 2.123343378353565\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {entropy_from_kde(kde_p)}')\n",
    "print(f'Entropy of q = {entropy_from_kde(kde_q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:40.190449Z",
     "start_time": "2020-07-31T03:56:40.131586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.517390423126535\n",
      "Entropy of q = 2.1120728496363306\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {entropy_from_density_with_support(pdf_p, p_min, p_max)}')\n",
    "print(f'Entropy of q = {entropy_from_density_with_support(pdf_q, q_min, q_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Entropy of a Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:40.228574Z",
     "start_time": "2020-07-31T03:56:40.191930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.5175508218727822\n",
      "Entropy of q = 2.112085713764618\n"
     ]
    }
   ],
   "source": [
    "def theoretical_entropy_of_normal_distribution(mu: float, sigma: float, log_fun: tp.Callable = np.log) -> float:\n",
    "    return 0.5 * (1.0 + log_fun(2 * np.pi * sigma**2))\n",
    "\n",
    "print(f'Entropy of p = {theoretical_entropy_of_normal_distribution(mu_p, sigma_p)}')\n",
    "print(f'Entropy of q = {theoretical_entropy_of_normal_distribution(mu_q, sigma_q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:40.268189Z",
     "start_time": "2020-07-31T03:56:40.230965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 0.639031859650177\n",
      "Entropy of q = 1.0888999753452238\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {discrete_entropy(multinomial_sample_p)}')\n",
    "print(f'Entropy of q = {discrete_entropy(multinomial_sample_q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy of a distribution $q$ relative to a distribution $p$ is defined as  \n",
    "\n",
    "$H_q(p) = - \\mathbb{E}_p \\left[ \\log_{\\text{base}} q \\right]$.\n",
    "\n",
    "With a base of 2, the cross-entropy of $q$ relative to $p$ is the average number of bits required to encode the signal in $p$ using a code optimized for the signal in $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy from Samples (via Statsmodels KDE Objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:40.738979Z",
     "start_time": "2020-07-31T03:56:40.269946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy of p relative to q = 2.9007913519550272\n",
      "Cross Entropy of q relative to p = 2.306094354037839\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Entropy of p relative to q = {cross_entropy_from_samples(sample_p, sample_q, discrete=False)}')\n",
    "print(f'Cross Entropy of q relative to p = {cross_entropy_from_samples(sample_q, sample_p, discrete=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:43.995286Z",
     "start_time": "2020-07-31T03:56:40.740765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 ms ± 12.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cross_entropy_from_kde(kde_p, kde_q), cross_entropy_from_kde(kde_q, kde_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:44.448376Z",
     "start_time": "2020-07-31T03:56:43.997577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy of p relative to q = 2.9007913519550272\n",
      "Cross Entropy of q relative to p = 2.306094354037839\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Entropy of p relative to q = {cross_entropy_from_kde(kde_p, kde_q)}')\n",
    "print(f'Cross Entropy of q relative to p = {cross_entropy_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:44.518454Z",
     "start_time": "2020-07-31T03:56:44.450634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy of p relative to q = 2.86176079907269\n",
      "Cross Entropy of q relative to p = 2.295328590629144\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Entropy of p relative to q = {cross_entropy_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Cross Entropy of q relative to p = {cross_entropy_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:44.914306Z",
     "start_time": "2020-07-31T03:56:44.520198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy of p relative to q = 0.9738271463645112\n",
      "Cross Entropy of q relative to p = 1.4708084763221114\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Entropy of p relative to q = {discrete_cross_entropy(multinomial_sample_p, multinomial_sample_q)}')\n",
    "print(f'Cross Entropy of q relative to p = {discrete_cross_entropy(multinomial_sample_q, multinomial_sample_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Entropy (Kullback-Leibler Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative entropy or Kullback-Leibler divergence measures the dispersion of two probability distributions $P$ and $Q$. It is defined as the difference between the cross entropy of $q$ relative to $p$ and the entropy of $p$\n",
    "\n",
    "$D_{KL} (P||Q) = \\mathbb{E}_p \\left[ \\log_{\\text{base}} \\left( \\frac{p}{q} \\right) \\right] = H_q(p) - H(p)$.\n",
    "\n",
    "With a base of 2, it can be interpreted as the average number of additional bits required to encode the signal in $p$ using a code optimized for the signal in $q$ over and above the number of bits required by the optimal code for $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy from Samples (via Statsmodels KDE Objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:45.504577Z",
     "start_time": "2020-07-31T03:56:44.916068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Entropy of p relative to q = 0.3696813653031077\n",
      "Relative Entropy of q relative to p = 0.18274894857179375\n"
     ]
    }
   ],
   "source": [
    "print(f'Relative Entropy of p relative to q = {relative_entropy_from_samples(sample_p, sample_q, discrete=False)}')\n",
    "print(f'Relative Entropy of q relative to p = {relative_entropy_from_samples(sample_q, sample_p, discrete=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:46.081252Z",
     "start_time": "2020-07-31T03:56:45.506134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Entropy of p relative to q = 0.3696813653031077\n",
      "Relative Entropy of q relative to p = 0.18274894857179375\n"
     ]
    }
   ],
   "source": [
    "print(f'Relative Entropy of p relative to q = {relative_entropy_from_kde(kde_p, kde_q)}')\n",
    "print(f'Relative Entropy of q relative to p = {relative_entropy_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:46.142660Z",
     "start_time": "2020-07-31T03:56:46.082739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Entropy from p to q = 0.34437037594615566\n",
      "Relative Entropy from q to p = 0.1832428925442867\n"
     ]
    }
   ],
   "source": [
    "print(f'Relative Entropy from p to q = {relative_entropy_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Relative Entropy from q to p = {relative_entropy_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Relative Entropy for Normal Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:46.177322Z",
     "start_time": "2020-07-31T03:56:46.144357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Entropy from p to q = 0.34453489189183556\n",
      "Relative Entropy from q to p = 0.18324288588594217\n"
     ]
    }
   ],
   "source": [
    "def relative_entropy_between_normal_distributions(mu_1, sigma_1, mu_2, sigma_2, log_fun: tp.Callable = np.log):\n",
    "    return ((mu_1 - mu_2)**2 + sigma_1**2 - sigma_2**2 ) / (2 * sigma_2**2) + log_fun(sigma_2/sigma_1)\n",
    "\n",
    "print(f'Relative Entropy from p to q = {relative_entropy_between_normal_distributions(mu_p, sigma_p, mu_q, sigma_q)}')\n",
    "print(f'Relative Entropy from q to p = {relative_entropy_between_normal_distributions(mu_q, sigma_q, mu_p, sigma_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:46.216253Z",
     "start_time": "2020-07-31T03:56:46.178950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Entropy of p relative to q = 0.3347952867143343\n",
      "Relative Entropy of q relative to p = 0.3819085009768876\n"
     ]
    }
   ],
   "source": [
    "print(f'Relative Entropy of p relative to q = {discrete_relative_entropy(multinomial_sample_p, multinomial_sample_q)}')\n",
    "print(f'Relative Entropy of q relative to p = {discrete_relative_entropy(multinomial_sample_q, multinomial_sample_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jensen-Shannon divergence, a symmetric measure of the divergence of probability distributions, is defined as\n",
    "\n",
    "$JSD(p||q) = \\frac{1}{2} D_{KL} (p||m) + \\frac{1}{2} D_{KL} (q||m)$, \n",
    "\n",
    "where $m = \\frac{1}{2} \\left( p + q \\right)$.\n",
    "\n",
    "For base 2, the JSD is bounded between 0 and 1. For base $e$, it is bounded between $0$ and $\\ln(2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Divergence from Samples (via Statsmodels KDE Objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:48.168485Z",
     "start_time": "2020-07-31T03:56:46.218072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.052550634833070334\n",
      "Jensen-Shannon Divergence between q and p = 0.052550634833070334\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {jensen_shannon_divergence_from_samples(sample_p, sample_q, discrete=False)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {jensen_shannon_divergence_from_samples(sample_q, sample_p, discrete=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Divergence from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:50.119943Z",
     "start_time": "2020-07-31T03:56:48.176238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.052550634833070334\n",
      "Jensen-Shannon Divergence between q and p = 0.052550634833070334\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {jensen_shannon_divergence_from_kde(kde_p, kde_q)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {jensen_shannon_divergence_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Divergence from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:50.286626Z",
     "start_time": "2020-07-31T03:56:50.123680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.05290044224944191\n",
      "Jensen-Shannon Divergence between q and p = 0.05290044224944191\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {jensen_shannon_divergence_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {jensen_shannon_divergence_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Divergence from Statsmodels KDE Objects in Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:52.790953Z",
     "start_time": "2020-07-31T03:56:50.288603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.07581454026923815\n",
      "Jensen-Shannon Divergence between q and p = 0.07581454026923815\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {jensen_shannon_divergence_from_kde(kde_p, kde_q, base=2.0)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {jensen_shannon_divergence_from_kde(kde_q, kde_p, base=2.0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation Function Specific to Discrete Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:52.833513Z",
     "start_time": "2020-07-31T03:56:52.792870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.0863046217355343\n",
      "Jensen-Shannon Divergence between q and p = 0.0863046217355343\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {discrete_jensen_shannon_divergence(multinomial_sample_p, multinomial_sample_q)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {discrete_jensen_shannon_divergence(multinomial_sample_q, multinomial_sample_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic calculation functionality covering samples from continuous as well as discrete distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:52.877694Z",
     "start_time": "2020-07-31T03:56:52.835345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.0863046217355343\n",
      "Jensen-Shannon Divergence between q and p = 0.0863046217355343\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {jensen_shannon_divergence_from_samples(multinomial_sample_p, multinomial_sample_q, discrete=True)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {jensen_shannon_divergence_from_samples(multinomial_sample_q, multinomial_sample_p, discrete=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information is a measure of measure of mutual dependence of random variables that goes beyond linear dependence measured by correlation. It is defined as the KL-divergence between the joint density of two random variables $x$ and $y$ and the product of their marginal densities, i.e.  \n",
    "\n",
    "$I(X; Y) = D_KL(p_{x, y}|| p_x \\otimes p_y) = E_{p_{x, y}} \\left[ \\log_{\\text{base}} \\left( \\frac{p_{x, y} (x, y)}{p_x(x) p_y(y)} \\right) \\right]$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Artificial Data from a Bi-Variate Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:56:52.940599Z",
     "start_time": "2020-07-31T03:56:52.879443Z"
    }
   },
   "outputs": [],
   "source": [
    "# set parameters of the normal distributions x and y\n",
    "mu_x = 2\n",
    "sigma_x = 3\n",
    "mu_y = 1\n",
    "sigma_y = 2\n",
    "rho = 0.5\n",
    "\n",
    "# draw 1000 samples from each normal distribution\n",
    "n = 10000\n",
    "z = np.random.randn(n)\n",
    "sample_x = mu_x + sigma_x * z\n",
    "sample_y = mu_y + sigma_y * (rho * z + np.sqrt(1.0 - rho**2) * np.random.randn(n))\n",
    "\n",
    "# fit a non-parametric density estimate for both distributions\n",
    "kde_x = sm.nonparametric.KDEUnivariate(sample_x)\n",
    "kde_y = sm.nonparametric.KDEUnivariate(sample_y)\n",
    "kde_x.fit() # Estimate the densities\n",
    "kde_y.fit() # Estimate the densities\n",
    "kde_xy = sp.stats.gaussian_kde([sample_x, sample_y])\n",
    "\n",
    "# construct exact normal densities for x and y\n",
    "pdf_x = lambda x: sp.stats.norm.pdf(x, mu_x, sigma_x)\n",
    "pdf_y = lambda y: sp.stats.norm.pdf(y, mu_y, sigma_y)\n",
    "pdf_xy = sp.stats.multivariate_normal(mean=[mu_x, mu_y], cov=[[sigma_x**2, rho * sigma_x * sigma_y], [rho * sigma_x * sigma_y, sigma_y**2]]).pdf\n",
    "\n",
    "# # compute support for kernel density estimates\n",
    "x_min = min(kde_x.support)\n",
    "x_max = max(kde_x.support)\n",
    "y_min = min(kde_y.support)\n",
    "y_max = max(kde_y.support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information from Samples (via Statsmodels KDE Objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:57:26.465180Z",
     "start_time": "2020-07-31T03:56:52.942483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information of x and y = 0.14540631373336696\n"
     ]
    }
   ],
   "source": [
    "print(f'Mutual Information of x and y = {mutual_information_from_samples(sample_x, sample_y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:57:57.222125Z",
     "start_time": "2020-07-31T03:57:26.466817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information of x and y = 0.14540631373336696\n"
     ]
    }
   ],
   "source": [
    "print(f'Mutual Information of x and y = {mutual_information_from_kde(kde_x, kde_y, kde_xy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:57:59.131119Z",
     "start_time": "2020-07-31T03:57:57.223582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information of x and y = 0.14384103152628203\n"
     ]
    }
   ],
   "source": [
    "print(f'Mutual Information of x and y = {mutual_information_from_densities_with_support(pdf_x, pdf_y, pdf_xy, x_min=-20, x_max=20, y_min=-20, y_max=20)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Mutual Information of Bi-Variate Normal Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:57:59.163352Z",
     "start_time": "2020-07-31T03:57:59.132515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information of x and y = 0.14384103622589045\n"
     ]
    }
   ],
   "source": [
    "def mutual_information_for_bivariate_normal_distribution(rho: float, \n",
    "                                                         log_fun: tp.Callable = np.log) -> float:\n",
    "    return - 0.5 * np.log(1.0 - rho**2)\n",
    "\n",
    "print(f'Mutual Information of x and y = {mutual_information_for_bivariate_normal_distribution(rho)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct two discrete samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:57:59.197908Z",
     "start_time": "2020-07-31T03:57:59.165032Z"
    }
   },
   "outputs": [],
   "source": [
    "discrete_sample_x = np.array([1, 1, 3, 1, 2, 3])\n",
    "discrete_sample_y = np.array([1, 1, 1, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:57:59.549553Z",
     "start_time": "2020-07-31T03:57:59.199548Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mutual information of x and y is 0.5493061443340548\n"
     ]
    }
   ],
   "source": [
    "print(f'The mutual information of x and y is {discrete_mutual_information(discrete_sample_x, discrete_sample_y, base=np.e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:57:59.583343Z",
     "start_time": "2020-07-31T03:57:59.551049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mutual information of y and x is 0.5493061443340548\n"
     ]
    }
   ],
   "source": [
    "print(f'The mutual information of y and x is {discrete_mutual_information(discrete_sample_y, discrete_sample_x, base=np.e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint entropy of the random variables x and y with joint density $p_{x, y}$ is defined as  \n",
    "\n",
    "$H(X, Y) = - E_{p_{x, y}} \\left[ \\log_{\\text{base}} p_{x, y} (x, y) \\right]$.\n",
    "\n",
    "Joint entropy is symmetric, i.e.  \n",
    "\n",
    "$H(X, Y) = H(Y, X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:03.608382Z",
     "start_time": "2020-07-31T03:57:59.584775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint entropy of x and y = 4.475745990640665\n"
     ]
    }
   ],
   "source": [
    "joint_entropy_of_x_and_y = joint_entropy_from_samples(sample_x, sample_y)\n",
    "print(f'Joint entropy of x and y = {joint_entropy_of_x_and_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:03.640898Z",
     "start_time": "2020-07-31T03:58:03.610576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joint entropy of x and y is 1.3296613488547582\n"
     ]
    }
   ],
   "source": [
    "print(f'The joint entropy of x and y is {discrete_joint_entropy(discrete_sample_x, discrete_sample_y, base=np.e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional entropy of the random variable y given x with joint density $p_{x, y}$ and marginal density $p_x$ of $x$ is defined as  \n",
    "\n",
    "$H(Y|X) = - E_{p_{x, y}} \\left[ \\log \\frac{p_{x, y} (x, y)}{p_x(x)} \\right]$.  \n",
    "\n",
    "From this definition follows the change rule for conditional entropy\n",
    "\n",
    "\n",
    "$H(X, Y) = H(X) + H(Y|X)$.\n",
    "\n",
    "Switching the roles of $x$ and $y$ and using the symmetry of joint entropy, we obtain  \n",
    "\n",
    "$H(X, Y) = H(Y) + H(X|Y)$.\n",
    "\n",
    "Substracting second equation for joint entropyfrom the first and rearranging yields  \n",
    "\n",
    "$H(Y) - H(Y|X) = H(X) - H(X|Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:14.694029Z",
     "start_time": "2020-07-31T03:58:03.642453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional entropy of y given x = 1.9912929526616132\n"
     ]
    }
   ],
   "source": [
    "conditional_entropy_of_y_given_x = conditional_entropy_from_samples(sample_x, sample_y)\n",
    "print(f'Conditional entropy of y given x = {conditional_entropy_of_y_given_x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:24.841970Z",
     "start_time": "2020-07-31T03:58:14.696527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional entropy of x given y = 2.3857520195720445\n"
     ]
    }
   ],
   "source": [
    "conditional_entropy_of_x_given_y = conditional_entropy_from_samples(sample_y, sample_x)\n",
    "print(f'Conditional entropy of x given y = {conditional_entropy_of_x_given_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the chain rule of conditional entropy is satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:24.923287Z",
     "start_time": "2020-07-31T03:58:24.843722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(entropy_from_samples(sample_x) + conditional_entropy_of_y_given_x, joint_entropy_of_x_and_y, rtol=1e-2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:24.995415Z",
     "start_time": "2020-07-31T03:58:24.925091Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(entropy_from_samples(sample_y) + conditional_entropy_of_x_given_y, joint_entropy_of_x_and_y, rtol=1e-2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:32.421300Z",
     "start_time": "2020-07-31T03:58:24.996920Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional entropy of y given x (on gpu) = 1.9912435966457076\n"
     ]
    }
   ],
   "source": [
    "conditional_entropy_of_y_given_x_gpu = \\\n",
    "    continuous_conditional_entropy_from_samples_gpu(\n",
    "        sample_x, \n",
    "        sample_y, \n",
    "        maximum_number_of_elements_per_batch=-1)\n",
    "print(f'Conditional entropy of y given x (on gpu) = {conditional_entropy_of_y_given_x_gpu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:37.384352Z",
     "start_time": "2020-07-31T03:58:32.423384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional entropy of x given y (on gpu) = 2.385699213557811\n"
     ]
    }
   ],
   "source": [
    "conditional_entropy_of_x_given_y_gpu = continuous_conditional_entropy_from_samples_gpu(sample_y, sample_x)\n",
    "print(f'Conditional entropy of x given y (on gpu) = {conditional_entropy_of_x_given_y_gpu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:37.677112Z",
     "start_time": "2020-07-31T03:58:37.386846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conditional entropy of y given x is 0.31825708414740644\n"
     ]
    }
   ],
   "source": [
    "print(f'The conditional entropy of y given x is {discrete_conditional_entropy_of_y_given_x(discrete_sample_x, discrete_sample_y, base=np.e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the chain rule for conditional entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:37.712640Z",
     "start_time": "2020-07-31T03:58:37.679034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(discrete_entropy(discrete_sample_y) + discrete_conditional_entropy_of_y_given_x(discrete_sample_y, discrete_sample_x), discrete_joint_entropy(discrete_sample_x, discrete_sample_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T03:58:37.750939Z",
     "start_time": "2020-07-31T03:58:37.714443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(discrete_entropy(discrete_sample_x) + discrete_conditional_entropy_of_y_given_x(discrete_sample_x, discrete_sample_y), discrete_joint_entropy(discrete_sample_y, discrete_sample_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295.4755554199219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
