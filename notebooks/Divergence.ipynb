{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:44.966065Z",
     "start_time": "2020-06-15T20:55:44.945681Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:45.819870Z",
     "start_time": "2020-06-15T20:55:44.968115Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/michaelnowotny/anaconda3/envs/continuous_time_mcmc/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:45.854719Z",
     "start_time": "2020-06-15T20:55:45.822347Z"
    }
   },
   "outputs": [],
   "source": [
    "from divergence import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions and Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example considers two different normal distributions $p$ and $q$ with\n",
    "$p = N(2, 9)$ and $q = N(1, 4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:45.920171Z",
     "start_time": "2020-06-15T20:55:45.856803Z"
    }
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# set parameters of the normal distributions p and q\n",
    "mu_p = 2\n",
    "sigma_p = 3\n",
    "mu_q = 1\n",
    "sigma_q = 2\n",
    "\n",
    "# draw samples from each normal distribution\n",
    "n = 10000\n",
    "\n",
    "def draw_normal(mu, sigma, n: int, antithetic: bool = False):\n",
    "    z = np.random.randn(n)\n",
    "    if antithetic: \n",
    "        z = np.hstack((z, -z))\n",
    "    \n",
    "    return mu + sigma * z\n",
    "\n",
    "samples_p = draw_normal(mu_p, sigma_p, n=n, antithetic=True)\n",
    "samples_q = draw_normal(mu_q, sigma_q, n=n, antithetic=True)\n",
    "\n",
    "# fit a non-parametric density estimate for both distributions\n",
    "kde_p = sm.nonparametric.KDEUnivariate(samples_p)\n",
    "kde_q = sm.nonparametric.KDEUnivariate(samples_q)\n",
    "kde_p.fit()\n",
    "kde_q.fit()\n",
    "\n",
    "# construct exact normal densities for p and q\n",
    "pdf_p = lambda x: sp.stats.norm.pdf(x, mu_p, sigma_p)\n",
    "pdf_q = lambda x: sp.stats.norm.pdf(x, mu_q, sigma_q)\n",
    "\n",
    "# compute support for kernel density estimates\n",
    "p_min = min(kde_p.support)\n",
    "p_max = max(kde_p.support)\n",
    "q_min = min(kde_q.support)\n",
    "q_max = max(kde_q.support)\n",
    "combined_min = min(p_min, q_min)\n",
    "combined_max = max(p_max, q_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a probability distribution $p$ is defined as \n",
    "\n",
    "$H(X) = - \\mathbb{E}_p \\left[ \\log_{\\text{base}} p \\right]$, \n",
    "\n",
    "where $\\mathbb{E}_P$ denotes expectation with respect the probability distribution $p$. In information theory, the base of the logarithm is 2 and the interpretation of entropy is the average number of bits needed to optimally encode the signal represented by the distribution $p$. In this Python package we use $base=e$ instead. This results in the natural logarithm i.e. $\\log_e = \\ln$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy from Statsmodels KDE Objects (via Statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:46.329474Z",
     "start_time": "2020-06-15T20:55:45.921796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.531114322639585\n",
      "Entropy of q = 2.1233454054445\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {kde_p.entropy}')\n",
    "print(f'Entropy of q = {kde_q.entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy from Statsmodels KDE Objects (via Divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:46.992581Z",
     "start_time": "2020-06-15T20:55:46.331761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.5311099866509332\n",
      "Entropy of q = 2.1233433783535673\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {compute_entropy_from_kde(kde_p)}')\n",
    "print(f'Entropy of q = {compute_entropy_from_kde(kde_q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:47.056398Z",
     "start_time": "2020-06-15T20:55:46.994371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of p = 2.5173904231265363\n",
      "Entropy of q = 2.1120728496363306\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of p = {compute_entropy_from_density_with_support(pdf_p, p_min, p_max)}')\n",
    "print(f'Entropy of q = {compute_entropy_from_density_with_support(pdf_q, q_min, q_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy of a distribution $q$ relative to a distribution $p$ is defined as  \n",
    "\n",
    "$H(p, q) = - \\mathbb{E}_p \\left[ \\log_{\\text{base}} q \\right]$.\n",
    "\n",
    "With a base of 2, the cross-entropy of $q$ relative to $p$ is the average number of bits required to encode the signal in $p$ using a code optimized for the signal in $q$. As with the entropy, we use base $e$ in this package instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:47.674939Z",
     "start_time": "2020-06-15T20:55:47.059703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy of p relative to q = 2.9007913519550996\n",
      "Cross Entropy of q relative to p = 2.3060943540378385\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Entropy of p relative to q = {compute_cross_entropy_from_kde(kde_p, kde_q)}')\n",
    "print(f'Cross Entropy of q relative to p = {compute_cross_entropy_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:47.745829Z",
     "start_time": "2020-06-15T20:55:47.677000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy of p relative to q = 2.861760799072692\n",
      "Cross Entropy of q relative to p = 2.295328590629144\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Entropy of p relative to q = {compute_cross_entropy_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Cross Entropy of q relative to p = {compute_cross_entropy_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Entropy (Kullback-Leibler Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative entropy or Kullback-Leibler divergence measures the dispersion of two probability distributions $P$ and $Q$. It is defined as the difference between the cross entropy of $q$ relative to $p$ and the entropy of $p$\n",
    "\n",
    "$D_{KL} (P||Q) = \\mathbb{E}_p \\left[ \\log_{\\text{base}} \\left( \\frac{p}{q} \\right) \\right] = H(p, q) - H(p)$.\n",
    "\n",
    "With a base of 2, it can be interpreted as the average number of additional bits required to encode the signal in $p$ using a code optimized for the signal in $q$ over and above the number of bits required by the optimal code for $p$. In this package we are using the natural logarithm instead of a base 2 logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Entropy from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:48.502203Z",
     "start_time": "2020-06-15T20:55:47.747563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Entropy of p relative to q = 0.36968136530318224\n",
      "Relative Entropy of q relative to p = 0.18274894856540888\n"
     ]
    }
   ],
   "source": [
    "print(f'Relative Entropy of p relative to q = {compute_relative_entropy_from_kde(kde_p, kde_q)}')\n",
    "print(f'Relative Entropy of q relative to p = {compute_relative_entropy_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:48.583386Z",
     "start_time": "2020-06-15T20:55:48.504514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Entropy from p to q = 0.3443703759461555\n",
      "Relative Entropy from q to p = 0.18324289254428677\n"
     ]
    }
   ],
   "source": [
    "print(f'Relative Entropy from p to q = {compute_relative_entropy_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Relative Entropy from q to p = {compute_relative_entropy_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jensen-Shannon divergence, a symmetric measure of the divergence of probability distributions, is defined as\n",
    "\n",
    "$JSD(p||q) = \\frac{1}{2} D_{KL} (p||m) + \\frac{1}{2} D_{KL} (q||m)$, \n",
    "\n",
    "where $m = \\frac{1}{2} \\left( p + q \\right)$.\n",
    "\n",
    "For base 2, the JSD is bounded between 0 and 1. For base $e$, it is bounded between $0$ and $\\ln(2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon Divergence from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:51.215069Z",
     "start_time": "2020-06-15T20:55:48.585087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.0525506348426028\n",
      "Jensen-Shannon Divergence between q and p = 0.0525506348426028\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {compute_jensen_shannon_divergence_from_kde(kde_p, kde_q)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {compute_jensen_shannon_divergence_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon Divergence from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T20:55:51.517869Z",
     "start_time": "2020-06-15T20:55:51.218093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between p and q = 0.05290044224944204\n",
      "Jensen-Shannon Divergence between q and p = 0.05290044224944204\n"
     ]
    }
   ],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {compute_jensen_shannon_divergence_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {compute_jensen_shannon_divergence_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
