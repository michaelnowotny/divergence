{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:06.896794Z",
     "start_time": "2020-06-17T04:31:06.872418Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:07.936699Z",
     "start_time": "2020-06-17T04:31:06.899746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/michaelnowotny/anaconda3/envs/continuous_time_mcmc/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:07.974672Z",
     "start_time": "2020-06-17T04:31:07.939349Z"
    }
   },
   "outputs": [],
   "source": [
    "from divergence import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions and Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Artificial Sample from two Normal Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example considers two different normal distributions $p$ and $q$ with\n",
    "$p = N(2, 9)$ and $q = N(1, 4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.044548Z",
     "start_time": "2020-06-17T04:31:07.976766Z"
    }
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# set parameters of the normal distributions p and q\n",
    "mu_p = 2\n",
    "sigma_p = 3\n",
    "mu_q = 1\n",
    "sigma_q = 2\n",
    "\n",
    "# draw samples from each normal distribution\n",
    "n = 10000\n",
    "\n",
    "def draw_normal(mu, sigma, n: int, antithetic: bool = False):\n",
    "    z = np.random.randn(n)\n",
    "    if antithetic: \n",
    "        z = np.hstack((z, -z))\n",
    "    \n",
    "    return mu + sigma * z\n",
    "\n",
    "samples_p = draw_normal(mu_p, sigma_p, n=n, antithetic=True)\n",
    "samples_q = draw_normal(mu_q, sigma_q, n=n, antithetic=True)\n",
    "\n",
    "# fit a non-parametric density estimate for both distributions\n",
    "kde_p = sm.nonparametric.KDEUnivariate(samples_p)\n",
    "kde_q = sm.nonparametric.KDEUnivariate(samples_q)\n",
    "kde_p.fit()\n",
    "kde_q.fit()\n",
    "\n",
    "# construct exact normal densities for p and q\n",
    "pdf_p = lambda x: sp.stats.norm.pdf(x, mu_p, sigma_p)\n",
    "pdf_q = lambda x: sp.stats.norm.pdf(x, mu_q, sigma_q)\n",
    "\n",
    "# compute support for kernel density estimates\n",
    "p_min = min(kde_p.support)\n",
    "p_max = max(kde_p.support)\n",
    "q_min = min(kde_q.support)\n",
    "q_max = max(kde_q.support)\n",
    "combined_min = min(p_min, q_min)\n",
    "combined_max = max(p_max, q_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Sample from Multinomial Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.082958Z",
     "start_time": "2020-06-17T04:31:08.046346Z"
    }
   },
   "outputs": [],
   "source": [
    "multinomial_sample_q = np.array([1, 2, 3, 2, 3, 3, 3, 2, 1, 1])\n",
    "multinomial_sample_p = np.array([2, 2, 3, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.118558Z",
     "start_time": "2020-06-17T04:31:08.084617Z"
    }
   },
   "outputs": [],
   "source": [
    "# combined_sample = np.hstack((multinomial_sample_p, multinomial_sample_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.154165Z",
     "start_time": "2020-06-17T04:31:08.120346Z"
    }
   },
   "outputs": [],
   "source": [
    "# unique_combined = np.unique(combined_sample)\n",
    "# unique_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.189777Z",
     "start_time": "2020-06-17T04:31:08.157422Z"
    }
   },
   "outputs": [],
   "source": [
    "# unique_q, counts_q = np.unique(multinomial_sample_q, return_counts=True)\n",
    "# frequencies_q = counts_q / len(multinomial_sample_q)\n",
    "# print(f'unique_q = {unique_q}')\n",
    "# print(f'counts_q = {counts_q}')\n",
    "# print(f'frequencies_q = {frequencies_q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.225743Z",
     "start_time": "2020-06-17T04:31:08.192393Z"
    }
   },
   "outputs": [],
   "source": [
    "# unique_p, counts_p = np.unique(multinomial_sample_p, return_counts=True)\n",
    "# frequencies_p = counts_p / len(multinomial_sample_p)\n",
    "# print(f'unique_p = {unique_p}')\n",
    "# print(f'counts_p = {counts_p}')\n",
    "# print(f'frequencies_p = {frequencies_p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.261083Z",
     "start_time": "2020-06-17T04:31:08.227509Z"
    }
   },
   "outputs": [],
   "source": [
    "# realization_to_frequency_dict_q = dict(zip(unique_q, frequencies_q))\n",
    "# realization_to_frequency_dict_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.296306Z",
     "start_time": "2020-06-17T04:31:08.262651Z"
    }
   },
   "outputs": [],
   "source": [
    "# realization_to_frequency_dict_p = dict(zip(unique_p, frequencies_p))\n",
    "# realization_to_frequency_dict_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.330286Z",
     "start_time": "2020-06-17T04:31:08.298033Z"
    }
   },
   "outputs": [],
   "source": [
    "# combined_frequencies_q = np.array([realization_to_frequency_dict_q.get(realization, 0.0) \n",
    "#                                    for realization \n",
    "#                                    in unique_combined])\n",
    "# combined_frequencies_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.365732Z",
     "start_time": "2020-06-17T04:31:08.331883Z"
    }
   },
   "outputs": [],
   "source": [
    "# combined_frequencies_p = np.array([realization_to_frequency_dict_p.get(realization, 0.0) \n",
    "#                                    for realization \n",
    "#                                    in unique_combined])\n",
    "# combined_frequencies_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.402180Z",
     "start_time": "2020-06-17T04:31:08.367518Z"
    }
   },
   "outputs": [],
   "source": [
    "# def discrete_relative_entropy_2(sample_p: np.ndarray, \n",
    "#                               sample_q: np.ndarray, \n",
    "#                               log_fun: tp.Callable = np.log):\n",
    "#     combined_sample = np.hstack((sample_p, sample_q))\n",
    "#     unique_combined = np.unique(combined_sample)\n",
    "    \n",
    "#     unique_q, counts_q = np.unique(sample_q, return_counts=True)\n",
    "#     frequencies_q = counts_q / len(sample_q)\n",
    "#     realization_to_frequency_dict_q = dict(zip(unique_q, frequencies_q))\n",
    "    \n",
    "#     unique_p, counts_p = np.unique(sample_p, return_counts=True)\n",
    "#     frequencies_p = counts_p / len(sample_p)\n",
    "#     realization_to_frequency_dict_p = dict(zip(unique_p, frequencies_p))\n",
    "    \n",
    "#     combined_frequencies_q = np.array([realization_to_frequency_dict_q.get(realization, 0.0) \n",
    "#                                        for realization \n",
    "#                                        in unique_combined])\n",
    "\n",
    "#     combined_frequencies_p = np.array([realization_to_frequency_dict_p.get(realization, 0.0) \n",
    "#                                        for realization \n",
    "#                                        in unique_combined])\n",
    "\n",
    "#     return sp.stats.entropy(pk=combined_frequencies_p, qk=combined_frequencies_q)\n",
    "# #     return sp.stats.entropy(pk=combined_frequencies_q, qk=combined_frequencies_p)\n",
    "\n",
    "\n",
    "# def construct_frequencies(sorted_p_realizations: np.ndarray, \n",
    "#                           sorted_p_frequencies: np.ndarray, \n",
    "#                           sorted_q_realizations: np.ndarray, \n",
    "#                           sorted_q_frequencies: np.ndarray, \n",
    "#                           sorted_combined_realizations: np.ndarray):\n",
    "#     assert len(sorted_p_realizations) == len(sorted_p_frequencies)\n",
    "#     assert len(sorted_q_realizations) == len(sorted_q_frequencies)\n",
    "\n",
    "#     p_source_index = 0\n",
    "#     q_source_index = 0\n",
    "#     p_target_index = 0\n",
    "#     q_target_index = 0\n",
    "    \n",
    "#     p_frequencies = np.zeros((len(sorted_p_realizations, )))\n",
    "#     q_frequencies = np.zeros((len(sorted_p_realizations, )))\n",
    "    \n",
    "#     for combined_index in range(len(sorted_combined_realizations)):\n",
    "#         realization = sorted_combined_realizations[combined_index]\n",
    "\n",
    "#         print(f'combined_index = {combined_index}, realization = {realization}')\n",
    "#         if sorted_p_realizations[p_source_index] != realization:\n",
    "#             print(f'realization {realization} is not in p')\n",
    "#             if sorted_q_realizations[q_source_index] == realization:\n",
    "#                 print(f'but realization {realization} is in q')\n",
    "#                 q_source_index += 1\n",
    "#             continue\n",
    "            \n",
    "#         if sorted_p_frequencies[p_source_index] == 0.0:\n",
    "#             p_source_index += 1\n",
    "#             if sorted_q_realizations[q_source_index] == realization:\n",
    "#                 q_source_index += 1\n",
    "#             continue\n",
    "            \n",
    "#         if sorted_q_realizations[q_source_index] != realization or sorted_q_realizations[q_source_index] == 0.0:\n",
    "#             if sorted_p_frequencies[p_source_index] != 0.0:   # we know that is true\n",
    "#                 # if q(x) == 0 we must have p(x) == 0, which is not the case here\n",
    "#                 raise ValueError('q(x) is zero but p(x) is not')\n",
    "#             else:\n",
    "#                 continue\n",
    "        \n",
    "#         p_frequencies[p_target_index] = sorted_p_frequencies[p_source_index]\n",
    "#         q_frequencies[q_target_index] = sorted_q_frequencies[q_source_index]\n",
    "#         p_source_index += 1\n",
    "#         q_source_index += 1\n",
    "#         p_target_index += 1\n",
    "#         q_target_index += 1\n",
    "            \n",
    "#     return p_frequencies[:p_target_index], q_frequencies[:q_target_index]\n",
    "\n",
    "\n",
    "# def discrete_relative_entropy(sample_p: np.ndarray, \n",
    "#                               sample_q: np.ndarray, \n",
    "#                               log_fun: tp.Callable = np.log):\n",
    "#     combined_sample = np.hstack((sample_p, sample_q))\n",
    "#     unique_combined = np.unique(combined_sample)\n",
    "    \n",
    "#     unique_q, counts_q = np.unique(sample_q, return_counts=True)\n",
    "#     frequencies_q = counts_q / len(sample_q)\n",
    "#     print(f'unique_q = {unique_q}')\n",
    "    \n",
    "#     unique_p, counts_p = np.unique(sample_p, return_counts=True)\n",
    "#     frequencies_p = counts_p / len(sample_p)\n",
    "#     print(f'unique_p = {unique_p}')\n",
    "    \n",
    "#     combined_frequencies_p, combined_frequencies_q = \\\n",
    "#         construct_frequencies(sorted_p_realizations=unique_p, \n",
    "#                               sorted_q_realizations=unique_q, \n",
    "#                               sorted_q_frequencies=frequencies_q, \n",
    "#                               sorted_p_frequencies=frequencies_p, \n",
    "#                               sorted_combined_realizations=unique_combined)\n",
    "    \n",
    "#     print(f'combined_frequencies_p = {combined_frequencies_p}')\n",
    "#     print(f'combined_frequencies_q = {combined_frequencies_q}')\n",
    "    \n",
    "#     return np.sum(combined_frequencies_p * log_fun(combined_frequencies_p/combined_frequencies_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.436553Z",
     "start_time": "2020-06-17T04:31:08.403812Z"
    }
   },
   "outputs": [],
   "source": [
    "# discrete_relative_entropy_2(multinomial_sample_p, multinomial_sample_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.474392Z",
     "start_time": "2020-06-17T04:31:08.438300Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_index = 0, realization = 1\n",
      "realization 1 is not in p\n",
      "but realization 1 is in q\n",
      "combined_index = 1, realization = 2\n",
      "combined_index = 2, realization = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4158883083359672"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_relative_entropy(multinomial_sample_p, multinomial_sample_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.511201Z",
     "start_time": "2020-06-17T04:31:08.476271Z"
    }
   },
   "outputs": [],
   "source": [
    "# discrete_relative_entropy(multinomial_sample_q, multinomial_sample_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.546926Z",
     "start_time": "2020-06-17T04:31:08.513085Z"
    }
   },
   "outputs": [],
   "source": [
    "# def _counts(sample: np.ndarray):\n",
    "#     _, counts = np.unique(sample, return_counts=True)\n",
    "#     return counts\n",
    "\n",
    "\n",
    "# def _frequencies(sample: np.ndarray):\n",
    "#     return _counts(sample) / len(sample)\n",
    "\n",
    "\n",
    "# def discrete_entropy(sample: np.ndarray, log_fun: tp.Callable = np.log):\n",
    "#     frequencies = _frequencies(sample)\n",
    "#     return - np.sum(frequencies * log_fun(frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.583672Z",
     "start_time": "2020-06-17T04:31:08.548623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5709505944546684"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_entropy(multinomial_sample_q, log_fun=np.log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.620983Z",
     "start_time": "2020-06-17T04:31:08.585291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_entropy(multinomial_sample_p, log_fun=np.log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.658853Z",
     "start_time": "2020-06-17T04:31:08.622562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_entropy(multinomial_sample_p, log_fun=np.log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.924888Z",
     "start_time": "2020-06-17T04:31:08.660542Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_frequencies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-087ed9900987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultinomial_sample_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_frequencies' is not defined"
     ]
    }
   ],
   "source": [
    "sp.stats.entropy(_frequencies(multinomial_sample_q), base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.927897Z",
     "start_time": "2020-06-17T04:31:06.903Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.stats.entropy(_frequencies(multinomial_sample_p), base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a probability distribution $p$ is defined as \n",
    "\n",
    "$H(X) = - \\mathbb{E}_p \\left[ \\log_{\\text{base}} p \\right]$, \n",
    "\n",
    "where $\\mathbb{E}_P$ denotes expectation with respect the probability distribution $p$. In information theory, the base of the logarithm is 2 and the interpretation of entropy is the average number of bits needed to optimally encode the signal represented by the distribution $p$. \n",
    "\n",
    "Divergence defaults to $\\text{base}=e$, which results in the natural logarithm i.e. $\\log_e = \\ln$. This default choice can be overridden by specifying a different logarithmic function than the natural logarithm in the argument 'log_fun' during the entropy calculation. In particular, specifying $\\text{base}=2$ by setting 'log_fun=np.log2' results in the classical Shannon entropy expressed in bits, whereas specifying $\\text{base}=10$ by setting 'log_fun=np.log10' produces the entropy in decimal bits (dits or Hartleys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy from Statsmodels KDE Objects (via Statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.929211Z",
     "start_time": "2020-06-17T04:31:06.905Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Entropy of p = {kde_p.entropy}')\n",
    "print(f'Entropy of q = {kde_q.entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy from Statsmodels KDE Objects (via Divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.930412Z",
     "start_time": "2020-06-17T04:31:06.907Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Entropy of p = {compute_entropy_from_kde(kde_p)}')\n",
    "print(f'Entropy of q = {compute_entropy_from_kde(kde_q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.931455Z",
     "start_time": "2020-06-17T04:31:06.908Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Entropy of p = {compute_entropy_from_density_with_support(pdf_p, p_min, p_max)}')\n",
    "print(f'Entropy of q = {compute_entropy_from_density_with_support(pdf_q, q_min, q_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Entropy of a Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.932509Z",
     "start_time": "2020-06-17T04:31:06.910Z"
    }
   },
   "outputs": [],
   "source": [
    "def theoretical_entropy_of_normal_distribution(mu: float, sigma: float, log_fun: tp.Callable = np.log) -> float:\n",
    "    return 0.5 * (1.0 + log_fun(2 * np.pi * sigma**2))\n",
    "\n",
    "print(f'Entropy of p = {theoretical_entropy_of_normal_distribution(mu_p, sigma_p)}')\n",
    "print(f'Entropy of q = {theoretical_entropy_of_normal_distribution(mu_q, sigma_q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy of a distribution $q$ relative to a distribution $p$ is defined as  \n",
    "\n",
    "$H(p, q) = - \\mathbb{E}_p \\left[ \\log_{\\text{base}} q \\right]$.\n",
    "\n",
    "With a base of 2, the cross-entropy of $q$ relative to $p$ is the average number of bits required to encode the signal in $p$ using a code optimized for the signal in $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.933867Z",
     "start_time": "2020-06-17T04:31:06.912Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Cross Entropy of p relative to q = {compute_cross_entropy_from_kde(kde_p, kde_q)}')\n",
    "print(f'Cross Entropy of q relative to p = {compute_cross_entropy_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.934794Z",
     "start_time": "2020-06-17T04:31:06.913Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Cross Entropy of p relative to q = {compute_cross_entropy_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Cross Entropy of q relative to p = {compute_cross_entropy_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Entropy (Kullback-Leibler Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative entropy or Kullback-Leibler divergence measures the dispersion of two probability distributions $P$ and $Q$. It is defined as the difference between the cross entropy of $q$ relative to $p$ and the entropy of $p$\n",
    "\n",
    "$D_{KL} (P||Q) = \\mathbb{E}_p \\left[ \\log_{\\text{base}} \\left( \\frac{p}{q} \\right) \\right] = H(p, q) - H(p)$.\n",
    "\n",
    "With a base of 2, it can be interpreted as the average number of additional bits required to encode the signal in $p$ using a code optimized for the signal in $q$ over and above the number of bits required by the optimal code for $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Entropy from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.936021Z",
     "start_time": "2020-06-17T04:31:06.915Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Relative Entropy of p relative to q = {compute_relative_entropy_from_kde(kde_p, kde_q)}')\n",
    "print(f'Relative Entropy of q relative to p = {compute_relative_entropy_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Entropy from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.937307Z",
     "start_time": "2020-06-17T04:31:06.917Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Relative Entropy from p to q = {compute_relative_entropy_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Relative Entropy from q to p = {compute_relative_entropy_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Relative Entropy for Normal Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.938546Z",
     "start_time": "2020-06-17T04:31:06.918Z"
    }
   },
   "outputs": [],
   "source": [
    "def relative_entropy_between_normal_distributions(mu_1, sigma_1, mu_2, sigma_2, log_fun: tp.Callable = np.log):\n",
    "    return ((mu_1 - mu_2)**2 + sigma_1**2 - sigma_2**2 ) / (2 * sigma_2**2) + log_fun(sigma_2/sigma_1)\n",
    "\n",
    "print(f'Relative Entropy from p to q = {relative_entropy_between_normal_distributions(mu_p, sigma_p, mu_q, sigma_q)}')\n",
    "print(f'Relative Entropy from q to p = {relative_entropy_between_normal_distributions(mu_q, sigma_q, mu_p, sigma_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jensen-Shannon divergence, a symmetric measure of the divergence of probability distributions, is defined as\n",
    "\n",
    "$JSD(p||q) = \\frac{1}{2} D_{KL} (p||m) + \\frac{1}{2} D_{KL} (q||m)$, \n",
    "\n",
    "where $m = \\frac{1}{2} \\left( p + q \\right)$.\n",
    "\n",
    "For base 2, the JSD is bounded between 0 and 1. For base $e$, it is bounded between $0$ and $\\ln(2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon Divergence from Statsmodels KDE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.940027Z",
     "start_time": "2020-06-17T04:31:06.920Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {compute_jensen_shannon_divergence_from_kde(kde_p, kde_q)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {compute_jensen_shannon_divergence_from_kde(kde_q, kde_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon Divergence from Normal Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.941274Z",
     "start_time": "2020-06-17T04:31:06.922Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {compute_jensen_shannon_divergence_from_densities_with_support(pdf_p, pdf_q, combined_min, combined_max)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {compute_jensen_shannon_divergence_from_densities_with_support(pdf_q, pdf_p, combined_min, combined_max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon Divergence from Statsmodels KDE Objects in Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:31:08.942658Z",
     "start_time": "2020-06-17T04:31:06.924Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Jensen-Shannon Divergence between p and q = {compute_jensen_shannon_divergence_from_kde(kde_p, kde_q, log_fun=np.log2)}')\n",
    "print(f'Jensen-Shannon Divergence between q and p = {compute_jensen_shannon_divergence_from_kde(kde_q, kde_p, log_fun=np.log2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
